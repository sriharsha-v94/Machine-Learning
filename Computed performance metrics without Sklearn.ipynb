{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "\n",
    "## A. Compute performance metrics for the given data '5_a.csv'\n",
    " <pre>  <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)\n",
    "Note- Make sure that you arrange your probability scores in descending order while calculating AUC</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_a=pd.read_csv(\"D:/Applied AI/Assignments/Assignment 5/5_a.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yg8uUJvGAfCM"
   },
   "outputs": [],
   "source": [
    "df_a['y_pred']=df_a['proba'].apply(lambda x: 1 if x>=.5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(data):\n",
    "    count_tn=len(data[(data['y']==0) & (data['y_pred']==0)])\n",
    "    count_tp=len(data[(data['y']==1) & (data['y_pred']==1)]) #calculating tn,tp,fn,fp\n",
    "    count_fn=len(data[(data['y']==1) & (data['y_pred']==0)])\n",
    "    count_fp=len(data[(data['y']==0) & (data['y_pred']==1)])\n",
    "    return count_fn,count_fp,count_tn,count_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(data):\n",
    "    fn,fp,tn,tp=confusion_matrix(data)\n",
    "    precision=tp/(tp+fp)                    # calculating precision and recall\n",
    "    recall=tp/(tp+fn)                    \n",
    "    f1=2*((precision*recall)/(precision+recall))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data):\n",
    "    fn,fp,tn,tp=confusion_matrix(data)\n",
    "    acc=((tp+tn)/(tp+fp+fn+tn))          \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(data):\n",
    "    tpr_array=[]\n",
    "    fpr_array=[]\n",
    "    sort= data.sort_values(\"proba\",ascending=False) # sort sart based on probability scores\n",
    "    for i in range(0,len(sort)):\n",
    "        sort['y_pred']=np.where(sort['proba']>=sort.iloc[i]['proba'],1,0) # predicting the y based on each threshold\n",
    "        FN,FP,TN,TP=confusion_matrix(sort)    # for each threshold calculating confusion matrix\n",
    "        fpr_rate=FP/(TN+FP)\n",
    "        tpr_rate=TP/(TP+FN)\n",
    "        tpr_array.append(tpr_rate)\n",
    "        fpr_array.append(fpr_rate)\n",
    "    c=np.trapz(tpr_array, fpr_array)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALSE NEGATIVE : 0\n",
      "FALSE POSITIVE : 100\n",
      "TRUE NEGATIVE : 0\n",
      "TRUE POSITIVE : 10000\n"
     ]
    }
   ],
   "source": [
    "FN,FP,TN,TP=confusion_matrix(df_a)\n",
    "print(\"FALSE NEGATIVE :\",FN)\n",
    "print(\"FALSE POSITIVE :\",FP)\n",
    "print(\"TRUE NEGATIVE :\",TN)\n",
    "print(\"TRUE POSITIVE :\",TP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE : 0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "f1=f1_score(df_a)\n",
    "print(\"F1 SCORE :\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY VALUE : 0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "acc=accuracy(df_a)\n",
    "print('ACCURACY VALUE :',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC VALUE : 0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "auc=auc_score(df_a)\n",
    "print('AUC VALUE :',auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "\n",
    "\n",
    "## B. Compute performance metrics for the given data '5_b.csv'\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a>\n",
    "Note- Make sure that you arrange your probability scores in descending order while calculating AUC</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b=pd.read_csv(\"D:/Applied AI/Assignments/Assignment 5/5_b.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b['y_pred']=df_b['proba'].apply(lambda x: 1 if x>=.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALSE NEGATIVE : 45\n",
      "FALSE POSITIVE : 239\n",
      "TRUE NEGATIVE : 9761\n",
      "TRUE POSITIVE : 55\n"
     ]
    }
   ],
   "source": [
    "FN,FP,TN,TP=confusion_matrix(df_b)\n",
    "print(\"FALSE NEGATIVE :\",FN)\n",
    "print(\"FALSE POSITIVE :\",FP)\n",
    "print(\"TRUE NEGATIVE :\",TN)\n",
    "print(\"TRUE POSITIVE :\",TP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE : 0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "f1=f1_score(df_b)\n",
    "print(\"F1 SCORE :\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY VALUE : 0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "acc=accuracy(df_b)\n",
    "print('ACCURACY VALUE :',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xlLVa-cVAfCS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC VALUE : 0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "auc=auc_score(df_b)\n",
    "print(\"AUC VALUE :\",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "### C. Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data \n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=pd.read_csv(\"D:/Applied AI/Assignments/Assignment 5/5_c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold(data):\n",
    "    check=0\n",
    "    thresh=[]\n",
    "    A=[]\n",
    "    sorted= data.sort_values(\"prob\",ascending=False) # sorting data based on probability\n",
    "    for i in range(0,len(sorted)):\n",
    "        if check==(sorted.iloc[i]['prob']): # checking unique probability\n",
    "            continue\n",
    "        check=sorted.iloc[i]['prob'] \n",
    "        thresh.append(check)\n",
    "        sorted['y_pred']=np.where(sorted['prob']>=sorted.iloc[i]['prob'],1,0)\n",
    "        FN,FP,TN,TP=confusion_matrix(sorted) # calculating confusion matrix for each threshold\n",
    "        value=500*FN+100*FP\n",
    "        A.append(value)  # calculating the metric A\n",
    "    index=A.index(min(A)) # finding the index of A with minimium value\n",
    "    return thresh[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST THRESHOLD VALUE : 0.2300390278970873\n"
     ]
    }
   ],
   "source": [
    "best=best_threshold(df_c)\n",
    "print('BEST THRESHOLD VALUE :',best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "\n",
    "## D.</b></font> Compute performance metrics(for regression) for the given data 5_d.csv\n",
    "<pre>    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d=pd.read_csv(\"D:/Applied AI/Assignments/Assignment 5/5_d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sVOj-bF9AfCd"
   },
   "outputs": [],
   "source": [
    "def regression_metrics(data):\n",
    "    n=len(data)\n",
    "    data['ei']= data.apply(lambda x: abs(x['y'] - x['pred']), axis=1) # calculating absolute differnce between Y and y^\n",
    "    data['mse']= data['ei'].apply(lambda x: x*x) # calculating the squares of ei\n",
    "    total=data['mse'].sum()\n",
    "    mse=total/n\n",
    "    mape=(data['ei'].sum())/(data['y'].sum())\n",
    "    mean=(data['y'].sum())/n # calculating simple mean of yi's\n",
    "    ssres=data['mse'].sum()\n",
    "    data['sstotal']= data.apply(lambda x: (x['y'] - mean), axis=1)\n",
    "    data['sstotal']= data['sstotal'].apply(lambda x: x*x)\n",
    "    sstotal=data['sstotal'].sum()\n",
    "    rsquared=1-(ssres/sstotal)\n",
    "    return mse,mape,rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uRhL1pheAfCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN SQUARED ERROR : 177.16569974554707\n",
      "MEAN ABSOLUTE PERCENTAGE ERROR : 12.91202994009687\n",
      "R SQUARED : 0.9563582786990937\n"
     ]
    }
   ],
   "source": [
    "mse,mape,rsquared=regression_metrics(df_d)\n",
    "print('MEAN SQUARED ERROR :',mse)\n",
    "print('MEAN ABSOLUTE PERCENTAGE ERROR :',mape*100)\n",
    "print('R SQUARED :',rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
